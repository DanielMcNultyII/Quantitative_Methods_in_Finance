{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black Litterman (BL) Function\n",
    "\n",
    "Based on [_Sector Weighting: A Detailed Implementation of Black-Litterman_](http://kpei.github.io/bl-sector-ssif/bl.html) by Kevin Pei, Senior Portfolio Analyst for Sprott Student Investment Fund. The R code provided by this source was converted to Python code, then modified to improve flexibility and remove instances of hard coding in order to allow for use in a trading strategy.\n",
    "\n",
    "The specific parts of this algorithm are explained in more detail within the \"Black-Litterman Code Explained\" files in this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup\n",
    "## *Load Necessary Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv as solver\n",
    "import pandas as pd\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Define Helper Functions to be Used within the BL Function*\n",
    "\n",
    "These functions include the impact and effect matrix generators, as well as the uncertainty matrix optimization formula to be used in the Black-Litterman function defined later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impact_maker creates the impact matrix from the level output.\n",
    "# lvl_ret is a dictionary of levels and their corresponding expected return\n",
    "def impact_maker(vec, lvl_ret={0:-0.04719978, 1:-0.02474642, 2:-0.014198, 3:-0.00443905, 4:0.00456932,\n",
    "                               5:0.01424941, 6:0.0241984, 7:0.04794779}):\n",
    "    # Find the minimum element of the input vector\n",
    "    m_vec = min(vec)\n",
    "    # For each element in the input vector that is not the minimum, make a vector of the expected returns\n",
    "    # corresponding to the element\n",
    "    q = [lvl_ret[i] for i in vec if i!=m_vec]\n",
    "    # Return the impact vector\n",
    "    return(q)\n",
    "\n",
    "# effect_maker creates the effect matrix from the level output\n",
    "def effect_maker(vec):\n",
    "    # Determine the minimum element of vec\n",
    "    m_vec = min(vec)\n",
    "    # Generate empty matrix to hold the views. Rows are the number of entries not equal to the minimum level, \n",
    "    # columns are the total number of entries in vec.\n",
    "    p = np.zeros((len(vec)-vec.count(m_vec),len(vec)))\n",
    "    \n",
    "    # Create dummy variables ind and j to keep track of the index of vec we are looking at and the row of the\n",
    "    # effect matrix we are interested in respectively\n",
    "    ind=0\n",
    "    j=0\n",
    "    # Loop through all the elements in vec\n",
    "    for elem in vec:\n",
    "        # If the current element is equal to the minimum, do nothing but add 1 to ind\n",
    "        if elem == m_vec:\n",
    "            ind=ind+1\n",
    "            continue\n",
    "        # If the current element isn't equal to the minimum\n",
    "        else:\n",
    "            # Create a list of the differences between the current element and any element in the vector that\n",
    "            # is less than it. Otherwise, let the input entry be 0. \n",
    "            cur_row = [i-elem if i<elem else 0.0 for i in vec]\n",
    "            # Find the denominator of the weights for current the effect matrix row by summing the absolute\n",
    "            # values of all the cur_row elements\n",
    "            denom = sum(np.absolute(cur_row))\n",
    "            # Divide all the cur_row elements by denom\n",
    "            cur_row = [i/denom for i in cur_row]\n",
    "            # Set the element in the cur_row corresponding to the current element being looked at to 1\n",
    "            cur_row[ind] = 1\n",
    "            # Input the row into the effect matrix\n",
    "            p[j]=cur_row \n",
    "            # Increment the dummy variables\n",
    "            j=j+1\n",
    "            ind=ind+1\n",
    "            \n",
    "    # Retern the generated effect matrix\n",
    "    return(np.matrix(p))\n",
    "\n",
    "# Define objective function to be minimized for step 5\n",
    "# ie. squared difference between the confidence weighting and Black-Litterman output weighting)\n",
    "def omega_solve(omega, w_conf_k, lam, sigma, tau, p, pi, q):\n",
    "    w_single =np.matmul(solver(lam*sigma),\n",
    "                        np.matmul(solver(solver(tau*sigma) + np.matmul(p.transpose(), (1/omega)*p)), \n",
    "                        (np.matmul(solver(tau*sigma), pi) + p.transpose() * (1/omega)*q)))\n",
    "    return(np.matrix.sum(np.square(np.subtract(w_conf_k, w_single))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Black-Litterman Function\n",
    "\n",
    "The main Black-Litterman function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bl(smp_out, r, mkt_cap, risk_free):\n",
    "    # Extract the date, stocks, and stock return levels from the sample lstm output\n",
    "    smp_out_date = smp_out[0]\n",
    "    smp_out_names = list(smp_out[1].keys())\n",
    "    smp_out_lvls = list(smp_out[1].values())\n",
    "    \n",
    "    # Create filtered log return dataframe\n",
    "    test_r = r.filter(items=smp_out_names).loc[(smp_out_date-timedelta(days=180)):smp_out_date]\n",
    "    \n",
    "    # Standard Deviation of Stock Returns\n",
    "    sigma = np.matrix(test_r.apply(np.std).values)\n",
    "    # Correlation Matrix\n",
    "    rho = test_r.corr()\n",
    "    # Covariance Matrix\n",
    "    cov_mat = np.matrix(rho * np.matmul(sigma.transpose(), sigma))\n",
    "    \n",
    "    # Market Cap Data for Chosen Stocks on Given Day\n",
    "    test_mc = mkt_cap.filter(items=smp_out_names).loc[smp_out_date]\n",
    "\n",
    "    # Calculate Weights from Market Caps\n",
    "    test_wts = test_mc.div(test_mc.sum(axis=0), axis=0)\n",
    "    test_wts\n",
    "    # Get Weekly Expected Return\n",
    "    r_total = np.matrix(np.matmul(np.matrix(test_r), test_wts.transpose())).transpose()\n",
    "    exp_wk_r = np.mean(r_total)*5\n",
    "\n",
    "    # 3 Month LIBOR Rate Converted to a Daily Rate by Dividing by 90\n",
    "    rf = float(risk_free.loc[smp_out_date])/(100.0*90.0)\n",
    "    # Variance of Total Return\n",
    "    var = np.var(r_total)\n",
    "    # Determine Lambda\n",
    "    lam = (exp_wk_r-rf)/var\n",
    "    \n",
    "    w_mkt = np.matrix(test_wts)\n",
    "    pi = lam * np.matmul(np.matrix(cov_mat), np.matrix(test_wts).transpose())\n",
    "    \n",
    "    # Create impact and effect matrices\n",
    "    Q = impact_maker(smp_out_lvls)\n",
    "    P = effect_maker(smp_out_lvls)\n",
    "    \n",
    "    # Calibrating Uncertainty ----------------------------------------------------------------------------\n",
    "    # STEP 1: Set confidence levels (HARDCODE ACCURACY OF NEURAL NET HERE)\n",
    "    cf_lvls = np.repeat(0.35, len(P))\n",
    "\n",
    "    K = P.shape[0]\n",
    "    tau=0.005\n",
    "    omega = np.zeros((K,K))\n",
    "    for i in range(0,K):\n",
    "        # STEP 2\n",
    "        bl_er_100 = pi + np.matmul(np.matmul(np.matmul(tau*cov_mat, P[i,:].transpose()), \n",
    "                                             solver(np.matmul(P[i,:] * tau, cov_mat*P[i,:].transpose()))),\n",
    "                                   Q[i]-P[i,:]*pi)\n",
    "        w_bl_100 = np.matmul(solver(lam*cov_mat), bl_er_100)\n",
    "\n",
    "        # STEP 3\n",
    "        tilt = (w_bl_100 - w_mkt.transpose())*cf_lvls[i]\n",
    "\n",
    "        # STEP 4\n",
    "        w_cf_k = w_mkt.transpose() + tilt\n",
    "\n",
    "        # STEP 5\n",
    "        m = np.arange(0.00001,0.001,0.00001)\n",
    "        d = 0\n",
    "        e = np.zeros(len(m))\n",
    "        ind = 0\n",
    "        min_e = 1000\n",
    "        for j in m:\n",
    "            e[d] = omega_solve(j, w_cf_k, lam, cov_mat, tau, P[i,:], pi, Q[i])       \n",
    "            d=d+1\n",
    "\n",
    "        omega[i,i] = m[np.where(e==min(e))]\n",
    "        \n",
    "        # Determine the Expected Returns from Black-Litterman using the Newly-Found Omega\n",
    "    bl_er=np.matmul(solver(solver(tau*cov_mat) + np.matmul(P.transpose(), np.matmul(solver(omega), P))), \n",
    "                          (np.matmul(solver(tau*cov_mat), pi) + np.matmul(P.transpose(), np.matmul(solver(omega), Q)).transpose()))\n",
    "    # Determine the Black-Litterman Weights from the Newly-Found Black-Litterman Expected Returns\n",
    "    bl_w = np.matmul(solver(lam*cov_mat), bl_er)\n",
    "    \n",
    "    # Return the Black-Litterman Weights found\n",
    "    return(bl_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Assumed Input Dataframes*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all log return data\n",
    "in_r = pd.read_csv('log_ret_data_final.csv',index_col='date', parse_dates=True)\n",
    "# Import market cap data\n",
    "in_mkt_cap = pd.read_csv('mark_cap_data_final.csv',index_col='date', parse_dates=True)\n",
    "# Read in and format 3 month LIBOR rate data\n",
    "in_risk_free = pd.read_csv('libor_rates_final.csv',index_col='date', parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Stock Forecast Output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample stock forecasting algorithm output\n",
    "smp_out = (pd.to_datetime('1995-07-03'),{'91704710':7, '15678210':0, '15102010':5, '53279110':5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Run Black-Litterman Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = bl(smp_out, in_r, in_mkt_cap, in_risk_free)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Check Black-Litterman Output*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 1.3571902 ],\n",
       "        [-0.63835761],\n",
       "        [-0.0360049 ],\n",
       "        [ 0.31717231]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
